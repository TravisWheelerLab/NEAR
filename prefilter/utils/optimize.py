import itertools
from pprint import pprint
from random import shuffle

import numpy as np
from argparse import ArgumentParser, Namespace
from hpc import SlurmCluster


def _parse_tunable_args(argument_name, tuning_list):
    """
    Creates a namespace from the list of arguments entered in at the command line.
    Supports parsing the definition of a hparam sweep and a single-value parameter.
    """
    if isinstance(tuning_list, list) and len(tuning_list) > 1:
        subparser = ArgumentParser(prog=argument_name)
        subparser.add_argument("--begin", type=float)
        subparser.add_argument("--end", type=float)
        subparser.add_argument("--log", action="store_true")
        mutex = subparser.add_mutually_exclusive_group()
        mutex.add_argument("--step", type=float)
        mutex.add_argument("--random", action="store_true")
        tuning_list = _process_argument_list(tuning_list)
        # _ to deal with two return values from parse_known_args
        command_specific_args, _ = subparser.parse_known_args(tuning_list)
    else:
        command_specific_args = Namespace()
        setattr(
            command_specific_args,
            argument_name,
            float(tuning_list[0]) if isinstance(tuning_list, list) else tuning_list,
        )

    return command_specific_args


def _add_tunable_arg(argument_parser, name, **kwargs):
    """
    tunable arguments:
    --params_to_tune begin end step log random
    Also accepts whatever keyword argument 'argument_parser.add_argument'
    accepts, in the form of **kwargs.
    """
    argument_parser.add_argument(
        f"{name}",
        nargs="+",
        help=f"usage: --{name} N_FFT or b/begin BEGIN e/end END s/step STEP log "
        "random",
        **kwargs,
    )

    return argument_parser


class SlurmSubmitter:
    """
    Class for doing hyperparameter tuning with slurm.
    a) assume the training routine is atomized and runs through
    a train() or main() function.
    b) ingest an argument parser and parse it to generate trials
       b.1) the argument parser will have two sub-groups: tunable and non-tunable.
       each tunable argument will have nargs="+" as its action and
    c) use subprocess.call() or subprocess.run()
    """

    def __init__(self, hparams):
        pass


def _process_argument_list(tuning_list):
    """
    Adds dashes before acceptable hyperparameter tuning arguments
    so they play nicely with argparse.
    """
    cmds = ("b", "begin", "e", "end", "log", "random", "s", "step")
    replacements = {"b": "begin", "e": "end", "l": "log", "r": "random", "s": "step"}
    for i in range(len(tuning_list)):
        cmd = tuning_list[i]
        if cmd in cmds:
            if cmd in replacements:
                cmd = replacements[cmd]
            tuning_list[i] = f"--{cmd}"
    return tuning_list


def _generate_argument_list(name, args, n_trials):
    """
    Generates argument arrays for each hyperparameter.
    Returns {name:array() or fixed_value}.
    Each training run is generated by ingesting a set of hparams
    """

    if hasattr(args, name):
        fixed_value = getattr(args, name)
        return {name: fixed_value}

    else:

        begin = args.begin
        end = args.end
        step = args.step
        log = args.log
        random = args.random

        # this function is garbage

        if begin >= end:
            raise ValueError(
                f"begin must be smaller than end for argument {name}, got begin: {begin}, end: {end}"
            )
        elif step is not None and step >= (end - begin):
            raise ValueError(
                f"step must be smaller than the difference between end and begin for argument {name}, "
                f"got begin: {begin}, end: {end}, step: {step}"
            )
        if log and step is not None:
            print(f"Got log == True for {name}, interpreting begin and end as powers")
            trials = np.logspace(
                begin, end, num=(10 ** end - 10 ** begin) / (10 ** step), base=10
            )
        elif log and random:
            trials = np.zeros(n_trials)
            i = 0
            while i < trials.shape[0]:
                trials[i] = np.random.choice(np.logspace(begin, end), 1)
                i += 1
        elif not log and random:
            trials = np.random.uniform(low=begin, high=end, size=n_trials)
        elif not log and step is not None:
            trials = np.arange(start=begin, stop=end, step=step)
        elif log:
            trials = np.logspace(start=begin, stop=end, num=n_trials)
        else:
            raise ValueError("unknown combination of arguments")

        return {name: trials}


class HyperOptParser:
    def __init__(self):
        self.args = None
        self.ap = ArgumentParser()
        self._tunable_registry = set()
        self.uniform_args = []
        self.random_args = []
        self.statics = []
        self.trials = None

    def add_tunable_argument(self, name, **kwargs):
        _add_tunable_arg(self.ap, name, **kwargs)
        self._tunable_registry.add(name.replace("--", "").replace("-", ""))

    def add_argument(self, *args, **kwargs):
        self.ap.add_argument(*args, **kwargs)

    def add_argument_group(self, *args, **kwargs):
        self.ap.add_argument_group(*args, **kwargs)

    def parse_args(self):

        self.args = self.ap.parse_args()
        if self.args.run:
            self.args.n_trials = 1

        if not hasattr(self.args, "n_trials") or self.args.n_trials is None:
            raise ValueError("you must specify n_trials to run at command line")

        argument_dict = vars(self.args)

        for k, v in argument_dict.items():

            if k in self._tunable_registry:

                cmd_args = _parse_tunable_args(k, v)

                if len(vars(cmd_args)) > 1:
                    if cmd_args.random:
                        self.random_args.append({k: cmd_args})
                    else:
                        name_to_trial_opts = _generate_argument_list(
                            k, cmd_args, self.args.n_trials
                        )
                        self.uniform_args.append(name_to_trial_opts)
                else:
                    # If the length is 1, then we know it's a static and it won't be
                    # put into a hparam tuning sweep
                    name_to_trial_opts = _generate_argument_list(
                        k, cmd_args, self.args.n_trials
                    )
                    self.statics.append(name_to_trial_opts)
            else:
                self.statics.append({k: v})

        self._generate_trials(self.args.n_trials)

        return self.args

    def _generate_trials(self, n_trials):

        if len(self.uniform_args) > 0:
            _uniform_args = []
            for elem in self.uniform_args:
                params = []
                for name, parameter_list in elem.items():
                    for param in parameter_list:
                        params.append({name: param})
                _uniform_args.append(params)

            self.trials = list(itertools.product(*_uniform_args))
            shuffle(self.trials)
            self.trials = self.trials[0:n_trials]

            trials_ = []
            for trial in self.trials:
                t = {}
                for param in trial:
                    t = {**t, **param}
                trials_.append(t)
            self.trials = trials_
        elif len(self.random_args) == 0 and len(self.uniform_args) == 0:
            self.trials = [{}]

        i = 0
        while i < n_trials and len(self.random_args) > 0:
            trial = self.trials[i]
            for elem in self.random_args:
                assert len(elem) == 1
                for param, cmd_list in elem.items():
                    name_to_trial = _generate_argument_list(
                        param, cmd_list, self.args.n_trials
                    )
                    sample = np.random.choice(name_to_trial[param], size=1)[0]
                    trial[param] = sample
                i += 1
        i = 0
        while i < len(self.trials):
            for non_tunable in self.statics:
                assert len(non_tunable) == 1
                for param, val in non_tunable.items():
                    self.trials[i][param] = val
            i += 1

        trials = []
        for trial in self.trials:
            trials.append(Namespace(**trial))
        self.trials = trials

    def generate_trials(self, n_trials):
        return self.trials


def train(args, kwargs):
    print(args, kwargs)
    print("diddly doo")


if __name__ == "__main__":

    ap = HyperOptParser()
    ap.add_argument("--n_trials", type=int)
    ap.add_argument("--data_dir", type=str)
    ap.add_argument("--log_path", type=str, default="tmp")
    ap.add_argument("--hpc_exp_number", type=int, default=0)
    ap.add_argument("--test_tube_exp_name", type=str, default="tmp")
    ap.add_argument("--run", action="store_true")
    ap.add_tunable_argument("--n_fft")
    ap.add_tunable_argument("--lr", default=1)
    ap.add_tunable_argument("--layers", default=1)
    ap.add_argument_group("tunable")
    hparams = ap.parse_args()

    cluster = SlurmCluster(
        hyperparam_optimizer=ap,
        log_path=hparams.log_path,
        python_cmd="python3",
    )
    cluster.per_experiment_nb_gpus = 0
    # SLURM Module to load.
    cluster.load_modules(
        [
            "python3",
        ]
    )

    # Add custom SLURM commands which show up as:
    # #comment
    # #SBATCH --cmd=value
    # ############
    cluster.add_slurm_cmd(
        cmd="partition", value="wheeler_lab_small_cpu", comment="partition"
    )

    # Set job compute details (this will apply PER set of hyperparameters.)
    cluster.per_experiment_nb_cpus = 1
    cluster.per_experiment_nb_nodes = 1

    # Each hyperparameter combination will use 200 cpus.
    cluster.optimize_parallel_cluster_cpu(
        # Function to execute:
        train,
        # Number of hyperparameter combinations to search:
        nb_trials=hparams.n_trials,
        job_name="first_tt_job",
        # This is what will display in the slurm queue:
        job_display_name="short_name",
    )
