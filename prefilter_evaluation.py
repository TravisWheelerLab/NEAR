# -*- coding: utf-8 -*-
"""prefilter_evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10iJFrIfNHUviGsCDX5J9sTBFDdC4vqKJ
"""
from google.colab import drive

drive.mount("/content/drive")

import time

import Bio
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from IPython.display import HTML, display
from prefilter_train_help.data import *


def progress(value, max=100):
    return HTML(
        """
        <progress
            value='{value}'
            max='{max}',
            style='width: 100%'
        >
            {value}
        </progress>
    """.format(
            value=value, max=max
        )
    )


out = display(progress(0, 100), display_id=True)
for ii in range(101):
    time.sleep(0.05)
    out.update(progress(ii, 100))

"""##Tommy Model"""

import pytorch_lightning as pl
import torch
import yaml


class ResConv(torch.nn.Module):
    def __init__(self, filters, kernel_size, padding, padding_mode):
        super(ResConv, self).__init__()

        self.padding = padding
        self.padding_mode = padding_mode
        self.kernel_size = kernel_size
        self.conv1 = torch.nn.Conv1d(
            filters, filters, kernel_size, padding=padding, padding_mode=padding_mode
        )
        self.act = torch.nn.ReLU()
        self.conv2 = torch.nn.Conv1d(
            filters, filters, kernel_size, padding=padding, padding_mode=padding_mode
        )

    def masked_forward(self, features, mask):
        x = self.conv1(features)
        mask = utils.mask_mask(mask)
        x = self.act(x)
        x = ~mask[:, None, :] * x
        x = self.conv2(x)
        mask = utils.mask_mask(mask)
        x = self.act(x)
        if self.padding == "valid":
            # two convolutions; so multiply half the kernel width by 2.
            # clearer than just self.kernel_size.
            features = features[
                :, :, 2 * (self.kernel_size // 2) : -2 * (self.kernel_size // 2)
            ]
        x = ~mask[:, None, :] * x
        return x + features, mask

    def forward(self, features):
        x = self.conv1(features)
        x = self.act(x)
        x = self.conv2(x)
        x = self.act(x)
        if self.padding == "valid":
            # two convolutions; so multiply half the kernel width by 2.
            # clearer than just self.kernel_size.
            features = features[
                :, :, 2 * (self.kernel_size // 2) : -2 * (self.kernel_size // 2)
            ]
        return x + features


def load_model(model_path, hyperparams, device):
    checkpoint = torch.load(model_path, map_location=torch.device(device))
    state_dict = checkpoint["state_dict"]
    if "training" in hyperparams:
        hyperparams["training"] = False
        if "apply_attention" not in hyperparams:
            hyperparams["apply_attention"] = False
        model = ResNet1d(**hyperparams).to(device)
    else:
        model = ResNet1d(**hyperparams, training=False).to(device)
    success = model.load_state_dict(state_dict)
    print(success)
    model.eval()
    success = 0
    return model, success


class ResNet1d(pl.LightningModule):
    def __init__(self, learning_rate, embed_msas, apply_attention, training=True):

        super(ResNet1d, self).__init__()

        self.learning_rate = learning_rate
        self.training = training
        self.embed_msas = embed_msas
        self.apply_attention = apply_attention

        if self.embed_msas:
            self.msa_transformer, _ = esm.pretrained.esm_msa1b_t12_100M_UR50S()
            self.msa_transformer.eval()
            self.msa_transformer.requires_grad_ = False

        if self.embed_msas:
            self.res_block_n_filters = 768
            self.msa_mlp = torch.nn.Sequential(
                *[
                    torch.nn.Conv1d(
                        self.res_block_n_filters, self.res_block_n_filters, 1
                    )
                ]
            )
            self.seq_mlp = torch.nn.Sequential(
                *[
                    torch.nn.Conv1d(
                        self.res_block_n_filters, self.res_block_n_filters, 1
                    )
                ]
            )
        else:
            self.res_block_n_filters = 256

        self.res_block_kernel_size = 3
        self.n_res_blocks = 10
        self.res_bottleneck_factor = 1
        self.padding = "same"
        self.padding_mode = "circular"

        self.log_interval = 100

        self.loss_func = None

        self._setup_layers()

        self.save_hyperparameters()

    def _setup_layers(self):

        self.embed = torch.nn.Embedding(27, self.res_block_n_filters)

        _list = []
        for _ in range(self.n_res_blocks):
            _list.append(
                ResConv(
                    self.res_block_n_filters,
                    kernel_size=self.res_block_kernel_size,
                    padding=self.padding,
                    padding_mode=self.padding_mode,
                )
            )

        self.embedding_trunk = torch.nn.Sequential(*_list)
        if self.apply_attention:
            self.transformer = torch.nn.TransformerEncoderLayer(
                self.res_block_n_filters,
                nhead=8,
                dim_feedforward=2 * self.res_block_n_filters,
            )

            self.pos_unc = model_utils.PositionalEncoding(self.res_block_n_filters)
        # could apply attention after max pooling then project back up to original dimension.

        mlp_list = [
            torch.nn.Conv1d(self.res_block_n_filters, self.res_block_n_filters, 1),
            torch.nn.ReLU(),
            torch.nn.Conv1d(self.res_block_n_filters, self.res_block_n_filters, 1),
        ]
        self.mlp = torch.nn.Sequential(*mlp_list)

    def _forward(self, x):
        x = self.embed(x)
        x = self.embedding_trunk(x.transpose(-1, -2))
        if self.apply_attention:
            x = x.transpose(1, 0).transpose(0, -1)
            x = self.transformer(x)
            x = x.transpose(1, 0).transpose(-1, -2)
        x = self.mlp(x)
        return x

    def _masked_forward(self, x, mask):
        x = self.embed(x).transpose(-1, -2)
        x = ~mask[:, None, :] * x
        for layer in self.embedding_trunk:
            x, mask = layer.masked_forward(x, mask)
        # mask here
        # removed a transpose for the LSTM.
        # point-wise mlp; no downsampling.
        # no need to mask the mask (kernel size 1).
        x = self.mlp(x)
        x = ~mask[:, None, :] * x
        return x, mask

    def forward(self, x, masks=None):
        if masks is not None:
            embeddings, masks = self._masked_forward(x, masks)
            return embeddings, masks
        else:
            embeddings = self._forward(x)
            return embeddings

    def _shared_step(self, batch):
        if self.embed_msas:
            if len(batch) == 3:
                msas, seqs, labels = batch
            else:
                msas, msa_labels, seqs, seq_labels, labels = batch
        elif len(batch) == 4:
            features, masks, labelvecs, labels = batch
        else:
            features, masks, labelvecs = batch

        if self.embed_msas:
            # each msa gets an _entire_ embedding
            # is there something weird about this?
            msa_embeddings = self.msa_transformer(
                msas, repr_layers=[12], return_contacts=False
            )["representations"][12].detach()
            # remove begin-of-sequence token.
            # msa_embeddings = msa_embeddings[:, :, 1:, :]
            # mean pool sequence embeddings across
            # msa dimension
            # msa_embeddings, _ = torch.max(msa_embeddings, dim=1)
            msa_embeddings = msa_embeddings[:, 0]
            # now apply two mlps.
            sequence_embeddings = (
                self.msa_transformer(seqs, repr_layers=[12], return_contacts=False)[
                    "representations"
                ][12]
                .detach()
                .squeeze()
            )
            # this should work well. If it doesn't something is up.
            sequence_embeddings = sequence_embeddings.transpose(-1, -2)
            msa_embeddings = msa_embeddings.transpose(-1, -2)

            msa_embeddings = self.msa_mlp(sequence_embeddings).transpose(-1, -2)
            sequence_embeddings = self.seq_mlp(sequence_embeddings).transpose(-1, -2)

            if self.global_step % self.log_interval == 0:
                with torch.no_grad():
                    _msa = torch.cat(torch.unbind(msa_embeddings, dim=0))
                    _seq = torch.cat(torch.unbind(sequence_embeddings, dim=0))
                    _msa = torch.nn.functional.normalize(_msa, dim=-1)
                    _seq = torch.nn.functional.normalize(_seq, dim=-1)

                    plt.imshow(torch.matmul(_msa, _seq.T).to("cpu").detach().numpy())
                    plt.colorbar()
                    fpath = (
                        f"{self.trainer.logger.log_dir}/image_{self.global_step}.png",
                    )
                    if os.path.isdir(self.trainer.logger.log_dir):
                        print(f"saving to {fpath}")
                        plt.savefig(
                            f"{self.trainer.logger.log_dir}/image_{self.global_step}.png",
                            bbox_inches="tight",
                        )
                    plt.close()
        else:
            if masks is not None:
                embeddings, masks = self.forward(features, masks)
            else:
                embeddings = self.forward(features)

        if self.embed_msas:
            _msa = torch.cat(torch.unbind(msa_embeddings, dim=0))
            _seq = torch.cat(torch.unbind(sequence_embeddings, dim=0))
            _msa = torch.nn.functional.normalize(_msa, dim=-1)
            _seq = torch.nn.functional.normalize(_seq, dim=-1)
            # now, drop ye old masked characters.
            x = torch.cat((_msa.unsqueeze(1), _seq.unsqueeze(1)), dim=1)
            loss = self.loss_func(x)

        else:
            e1, e2 = torch.split(
                embeddings.transpose(-1, -2), embeddings.shape[0] // 2, dim=0
            )
            e1 = torch.cat(torch.unbind(e1, dim=0))
            e2 = torch.cat(torch.unbind(e2, dim=0))
            e1 = torch.nn.functional.normalize(e1, dim=-1)
            e2 = torch.nn.functional.normalize(e2, dim=-1)
            if self.global_step % self.log_interval == 0:
                with torch.no_grad():
                    plt.imshow(torch.matmul(e1, e2.T).to("cpu").detach().numpy())
                    plt.colorbar()
                    fpath = (
                        f"{self.trainer.logger.log_dir}/image_{self.global_step}.png",
                    )
                    if os.path.isdir(self.trainer.logger.log_dir):
                        print(f"saving to {fpath}")
                        plt.savefig(
                            f"{self.trainer.logger.log_dir}/image_{self.global_step}.png",
                            bbox_inches="tight",
                        )
                    plt.close()
            loss = self.loss_func(torch.cat((e1.unsqueeze(1), e2.unsqueeze(1)), dim=1))

        return loss

    def training_step(self, batch, batch_nb):
        loss = self._shared_step(batch)
        return {"loss": loss}

    def validation_step(self, batch, batch_nb):
        loss = self._shared_step(batch)
        return {"val_loss": loss}

    def configure_optimizers(self):
        optim = torch.optim.Adam(
            filter_hits(lambda p: p.requires_grad, self.parameters()),
            lr=self.learning_rate,
        )
        # lr_schedule = torch.optim.lr_scheduler.StepLR(optim, step_size=15, gamma=0.5)
        return optim

    def training_epoch_end(self, outputs):
        train_loss = self.all_gather([x["loss"] for x in outputs])
        loss = torch.mean(torch.stack(train_loss))
        self.log("train_loss", loss)
        self.log("learning_rate", self.learning_rate)

    def on_train_start(self):
        self.log("hp_metric", self.learning_rate)

    def validation_epoch_end(self, outputs):
        val_loss = self.all_gather([x["val_loss"] for x in outputs])
        val_loss = torch.mean(torch.stack(val_loss))
        self.log("val_loss", val_loss)


hparams_path = "hparams.yaml"
with open(hparams_path, "r") as src:
    hparams = yaml.safe_load(src)
model = load_model("./epoch_17_2.024041.ckpt", hyperparams=hparams, device="cuda")
model = model[0]

with torch.no_grad():
    x = torch.randint(0, 20, (1, 160), device="cuda")
    print(model(x).shape)

import numpy as np

amino_alphabet = [c for c in "ACDEFGHILKMNPQRSTVWY"]
char_to_index = {c: i for i, c in enumerate(amino_alphabet)}


def _sanitize_sequence(sequence):
    """
    Remove bad/unknown/ambiguous characters from sequences.
    :param sequence:
    :type sequence:
    :return:
    :rtype:
    """
    sanitized = []
    for char in sequence:
        char = char.upper()
        if char in ("X", "U", "O"):
            sanitized.append("Q")
        elif char == "B":
            if int(2 * np.random.rand()) == 1:
                sanitized.append("D")
            else:
                sanitized.append("N")
        elif char == "Z":
            if int(2 * np.random.rand()) == 1:
                sanitized.append("E")
            else:
                sanitized.append("Q")
        else:
            sanitized.append(char)

    return sanitized


from Bio import SeqIO

queries = []
targets = []

query_names = []
target_names = []

with open("Q_benchmark2k30k.fa") as handle:
    for record in SeqIO.parse(handle, "fasta"):
        query_names.append(record.id)
        seq = _sanitize_sequence(record.seq)
        t_seq = torch.zeros(len(seq), dtype=torch.int64)
        for i in range(len(seq)):
            t_seq[i] = char_to_index[seq[i]]
        queries.append(t_seq.to("cuda"))

with open("T_benchmark2k30k.fa") as handle:
    for record in SeqIO.parse(handle, "fasta"):
        target_names.append(record.id)
        seq = _sanitize_sequence(record.seq)
        t_seq = torch.zeros(len(seq), dtype=torch.int64)
        for i in range(len(seq)):
            t_seq[i] = char_to_index[seq[i]]
        targets.append(t_seq.to("cuda"))

print(len(queries))
print(len(targets))

query_lengths = []
for q in queries:
    query_lengths.append(q.shape[0])

target_lengths = []
for t in targets:
    target_lengths.append(t.shape[0])

query_lengths = torch.tensor(query_lengths)
target_lengths = torch.tensor(target_lengths)

query_lengths, q_indices = torch.sort(query_lengths)
target_lengths, t_indices = torch.sort(target_lengths)

x = torch.arange(len(query_lengths))
plt.scatter(x, query_lengths)
plt.show()

x = torch.arange(len(target_lengths))
plt.scatter(x, target_lengths)
plt.show()

query_sequence = []
target_sequence = []

query_embeddings = []
target_embeddings = []

torch.cuda.empty_cache()

q_names = []
t_names = []

with torch.no_grad():
    for q in q_indices:
        # print(queries[q].shape)
        query_sequence.append(queries[q])
        e = model(queries[q].unsqueeze(0)).squeeze(0).to("cuda")
        e = torch.transpose(e, -1, -2)
        query_embeddings.append(e)
        q_names.append(query_names[q])

    for t in t_indices:
        target_sequence.append(targets[t])
        e = model(targets[t].unsqueeze(0)).squeeze(0).to("cuda")
        e = torch.transpose(e, -1, -2)
        target_embeddings.append(e)
        t_names.append(target_names[t])

print(len(query_embeddings), len(target_embeddings))
print(len(query_embeddings[0]), len(query_embeddings[-1]))
print(len(target_embeddings[0]), len(target_embeddings[-1]))
print(target_embeddings[0].shape, query_embeddings[0].shape)

print(targets[0].shape)

"""##Daniel Model"""


class DotProdModel(nn.Module):
    def __init__(self):
        super(DotProdModel, self).__init__()
        self.bias = nn.Parameter(torch.rand(1), requires_grad=True)

    # expects [batch, dim, seq_len]
    def Dot(self, A, B, activation=torch.sigmoid):
        matrix = torch.einsum("bei,bej->...bij", A, B)
        if activation is None:
            return matrix
        return activation(matrix + self.bias)

    # expects [batch, dim, seq_len]
    def L2Dist(self, A, B):
        return torch.cdist(torch.transpose(A, -1, -2), torch.transpose(B, -1, -2))


# Residual block class
# Takes [batch_size, channels, seq_len] and outputs the same shape
# Performs (num_layers) 1d convolutions and activations and then
# adds the output to the original input
"""
class ResidualBlock(nn.Module):
  def __init__(self, dims, num_layers, kernel_size, activation=nn.ELU, padding='same', padding_mode='circular', groups=1):
    super(ResidualBlock, self).__init__()

    layers = []

    for i in range(num_layers):
      layers.append(nn.Conv1d(dims, dims, kernel_size, padding=padding, padding_mode=padding_mode, groups=groups))
      layers.append(activation())


    self.layers = nn.Sequential(*layers)

  def forward(self, x):
    x = x + self.layers(x)
    return x
    """


class ResidualBlock(nn.Module):
    def __init__(
        self,
        dims,
        num_layers,
        kernel_size,
        activation=nn.ELU,
        padding="same",
        padding_mode="circular",
        groups=1,
    ):
        super(ResidualBlock, self).__init__()

        layers = []

        for i in range(num_layers):
            layers.append(
                nn.Conv1d(
                    dims,
                    dims,
                    kernel_size,
                    padding=padding,
                    padding_mode=padding_mode,
                    groups=groups,
                )
            )
            layers.append(activation())

        self.gate = nn.Conv1d(
            dims,
            dims,
            kernel_size,
            padding=padding,
            padding_mode=padding_mode,
            groups=groups,
        )
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        gate = torch.sigmoid(self.gate(x))
        x = (gate * x) + self.layers(x)
        return x


class ResNet(DotProdModel):
    def __init__(
        self,
        emb_dim,
        blocks,
        block_layers,
        start_emb=20,
        first_kernel=10,
        kernel_size=3,
        activation=nn.ELU,
        padding="same",
        padding_mode="circular",
        groups=1,
    ):
        super(ResNet, self).__init__()

        layers = []
        layers.append(
            nn.Conv1d(
                start_emb,
                emb_dim,
                first_kernel,
                padding=padding,
                padding_mode=padding_mode,
            )
        )
        layers.append(activation())
        for i in range(blocks):
            layers.append(
                ResidualBlock(
                    emb_dim,
                    block_layers,
                    kernel_size,
                    activation,
                    padding=padding,
                    padding_mode=padding_mode,
                    groups=groups,
                )
            )
            # layers.append(nn.Dropout(0.1))

        layers.append(
            nn.Conv1d(emb_dim, emb_dim, 1, padding=padding, padding_mode=padding_mode)
        )
        layers.append(activation())
        layers.append(
            nn.Conv1d(emb_dim, emb_dim, 1, padding=padding, padding_mode=padding_mode)
        )

        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)


from Bio import SeqIO

queries = []
targets = []

query_names = []
target_names = []

with open("Q_benchmark2k30k.fa") as handle:
    for record in SeqIO.parse(handle, "fasta"):
        query_names.append(record.id)
        queries.append(tensor_for_sequence(record.seq).to("cuda"))

with open("T_benchmark2k30k.fa") as handle:
    for record in SeqIO.parse(handle, "fasta"):
        target_names.append(record.id)
        targets.append(tensor_for_sequence(record.seq).to("cuda"))

print(len(queries))
print(len(targets))

query_lengths = []
for q in queries:
    query_lengths.append(q.shape[1])

target_lengths = []
for t in targets:
    target_lengths.append(t.shape[1])

query_lengths = torch.tensor(query_lengths)
target_lengths = torch.tensor(target_lengths)

query_lengths, q_indices = torch.sort(query_lengths)
target_lengths, t_indices = torch.sort(target_lengths)

x = torch.arange(len(query_lengths))
plt.scatter(x, query_lengths)
plt.show()

x = torch.arange(len(target_lengths))
plt.scatter(x, target_lengths)
plt.show()

model = torch.load("model.mod", map_location="cpu")
model.to("cuda")

from Bio import SeqIO

query_sequence = []
target_sequence = []

query_embeddings = []
target_embeddings = []

torch.cuda.empty_cache()

q_names = []
t_names = []

with torch.no_grad():
    for q in q_indices:
        query_sequence.append(queries[q])
        e = model(queries[q].unsqueeze(0)).squeeze(0).to("cuda")
        e = torch.transpose(e, -1, -2)
        query_embeddings.append(e)
        q_names.append(query_names[q])

    for t in t_indices:
        target_sequence.append(targets[t])
        e = model(targets[t].unsqueeze(0)).squeeze(0).to("cuda")
        e = torch.transpose(e, -1, -2)
        target_embeddings.append(e)
        t_names.append(target_names[t])

print(len(query_embeddings), len(target_embeddings))
print(len(query_embeddings[0]), len(query_embeddings[-1]))
print(len(target_embeddings[0]), len(target_embeddings[-1]))
print(target_embeddings[0].shape, query_embeddings[0].shape)

"""##Investigation int propper names"""

query_index = -1
target_index = -1
for i, name in enumerate(q_names):
    if name == "UniRef90_UPI00101D4B98":
        query_index = i
        break
for i, name in enumerate(t_names):
    if name == "UniRef90_A0A7W9F0J0":
        target_index = i
        break

print(query_index, target_index)

print(query_sequence[query_index])
print(target_sequence[target_index])
print(query_embeddings[query_index])
print(target_embeddings[target_index])
distances = torch.cdist(query_embeddings[query_index], target_embeddings[target_index])
print(distances.shape)
plt.imshow(distances.to("cpu"), interpolation="nearest")
plt.colorbar()
plt.show()

"""##Next"""


# Commented out IPython magic to ensure Python compatibility.
# %%time
# #for smallest query: 9.21 s
# #for largest query: 15.2s.
# # ez.pz.
# q = query_embeddings[-1]
# values = torch.zeros(len(target_embeddings))
# for i, t in enumerate(target_embeddings):
#   values[i] = torch.min(torch.cdist(q, t))
#
# values, _ = torch.sort(values)
# x = torch.arange(len(values))
# plt.scatter(x, values.to('cpu'))
# plt.show()


def filter_hits(
    queries, targets, query_names, target_names, threshold, start=0, end=100000000
):
    qdict = dict()
    out = display(progress(0, len(queries)), display_id=True)
    num_queries = min(end, len(queries))
    for i in range(start, num_queries):
        out.update(progress(i - start, num_queries - 1 - start))
        filtered_list = []
        qval = queries[i]  # torch.nn.functional.normalize(queries[i], dim=-1)
        for j in range(len(targets)):
            tval = targets[j]  # torch.nn.functional.normalize(targets[j], dim=-1)
            val = torch.min(torch.cdist(qval, tval))
            if val <= threshold:
                filtered_list.append((target_names[j], val, j))
        qdict[query_names[i]] = filtered_list
    return qdict


hits = filter_hits(
    query_embeddings, target_embeddings, q_names, t_names, 100.0, start=1000, end=1100
)

print(target_names)

print(len(hits))
for i, key in enumerate(hits):
    print(i, key)
    if i > 20:
        break
    for entry in hits[key]:
        print(entry)

with open("./drive/MyDrive/data/hits.txt", "w") as file:
    for key in hits:
        for entry in range(len(hits[key])):
            query = key
            target = hits[key][entry][0]
            distance = float(hits[key][entry][1])
            file.write(query + " " + target + " " + str(distance) + "\n")
